{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from  IPython.display import clear_output\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "import constants\n",
    "from constants import *\n",
    "import utils\n",
    "\n",
    "SIZE = None\n",
    "RUN_DEBUG = False\n",
    "LABELS = ['Hi-hat', 'Saxophone', 'Trumpet', 'Glockenspiel', 'Cello', 'Knock',\n",
    "       'Gunshot_or_gunfire', 'Clarinet', 'Computer_keyboard',\n",
    "       'Keys_jangling', 'Snare_drum', 'Writing', 'Laughter', 'Tearing',\n",
    "       'Fart', 'Oboe', 'Flute', 'Cough', 'Telephone', 'Bark', 'Chime',\n",
    "       'Bass_drum', 'Bus', 'Squeak', 'Scissors', 'Harmonica', 'Gong',\n",
    "       'Microwave_oven', 'Burping_or_eructation', 'Double_bass',\n",
    "       'Shatter', 'Fireworks', 'Tambourine', 'Cowbell', 'Electric_piano',\n",
    "       'Meow', 'Drawer_open_or_close', 'Applause', 'Acoustic_guitar',\n",
    "       'Violin_or_fiddle', 'Finger_snapping']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_from_folders(\n",
    "        predictions_folders, name_result, n_folds,\n",
    "        generate_predicted_labels=False,\n",
    "        name_result_for_generated_labels=None):\n",
    "    if generate_predicted_labels and name_result_for_generated_labels is None:\n",
    "        raise ValueError()\n",
    "    if not isinstance(n_folds, list):\n",
    "        n_folds = [n_folds] * len(predictions_folders)\n",
    "    if len(n_folds) != len(predictions_folders):\n",
    "        raise ValueError()\n",
    "    if not isinstance(n_folds[0], int):\n",
    "        raise ValueError()\n",
    "        \n",
    "    pred_list = []\n",
    "    for predictions_folder, n_folds_now in zip(predictions_folders, n_folds):\n",
    "        for i in range(n_folds_now):\n",
    "            pred_list.append(np.load(os.path.join(predictions_folder, 'test_predictions_%d.npy' % i)))\n",
    "    prediction = np.ones_like(pred_list[0])\n",
    "    print('count model in enseble: {}'.format(len(pred_list)))\n",
    "\n",
    "    for pred in pred_list:\n",
    "        prediction = prediction * pred\n",
    "    prediction = prediction ** (1.0 / len(pred_list))\n",
    "    np.save(os.path.join(constants.PREDICTIONS, name_result), prediction)\n",
    "    \n",
    "    if generate_predicted_labels:\n",
    "        top_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\n",
    "        predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "        test = pd.read_csv(os.path.join(constants.DATA, 'sample_submission.csv'))\n",
    "        test['label'] = predicted_labels\n",
    "        test[['fname', 'label']].to_csv(\n",
    "            os.path.join(constants.PREDICTIONS, name_result_for_generated_labels), index=False)\n",
    "        \n",
    "        # for debugging:\n",
    "        return pd.read_csv(os.path.join(\n",
    "            constants.PREDICTIONS, name_result_for_generated_labels)).head()\n",
    "    \n",
    "    \n",
    "def ensemble_from_files(predictions_files, name_result, weights):\n",
    "    if len(predictions_files) != len(weights):\n",
    "        raise ValueError\n",
    "    if abs(sum(weights) - 1) > 1e-6:\n",
    "        raise ValueError\n",
    "    pred_list = []\n",
    "    for predictions_file in predictions_files:\n",
    "        pred_list.append(np.load(os.path.join(constants.PREDICTIONS, predictions_file)))\n",
    "    prediction = np.ones_like(pred_list[0])\n",
    "        \n",
    "    for index, pred in enumerate(pred_list):\n",
    "        prediction = prediction * (pred  ** weights[index])\n",
    "    # prediction = prediction ** (1.0 / len(pred_list))\n",
    "    \n",
    "    top_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test = pd.read_csv(os.path.join(constants.DATA, 'sample_submission.csv'))\n",
    "    test['label'] = predicted_labels\n",
    "    test[['fname', 'label']].to_csv(\n",
    "        os.path.join(constants.PREDICTIONS, name_result), index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_two_stage_submission_file(\n",
    "        predictions_folders_1d,\n",
    "        predictions_folders_2d,\n",
    "        name_result,\n",
    "        weights,\n",
    "        n_folds_1d=10,\n",
    "        n_folds_2d=10):\n",
    "\n",
    "    name_predictions_for_1d = '1d_{}_{}'.format(\n",
    "        datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "        '.npy')\n",
    "    ensemble_from_folders(\n",
    "        predictions_folders_1d,\n",
    "        name_predictions_for_1d,\n",
    "        n_folds=n_folds_1d)\n",
    "    \n",
    "    name_predictions_for_2d = '2d_{}_{}'.format(\n",
    "        datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "        '.npy')\n",
    "    ensemble_from_folders(\n",
    "        predictions_folders_2d,\n",
    "        name_predictions_for_2d,\n",
    "        n_folds=n_folds_2d)\n",
    "\n",
    "    predictions_files = [name_predictions_for_1d, name_predictions_for_2d]\n",
    "    ensemble_from_files(\n",
    "        predictions_files, \n",
    "        name_result=name_result,\n",
    "        weights=weights)\n",
    "    \n",
    "    for name_to_remove in [name_predictions_for_1d, name_predictions_for_2d]:\n",
    "        os.remove(os.path.join(constants.PREDICTIONS, name_to_remove))\n",
    "    \n",
    "    # for debugging:\n",
    "    return pd.read_csv(os.path.join(constants.PREDICTIONS, name_result)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make two stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count model in enseble: 9\n",
      "count model in enseble: 10\n",
      "1d_2d_final_two_stage_0.5_0.5.csv\n"
     ]
    }
   ],
   "source": [
    "weight_1d = 0.5\n",
    "weight_2d = 0.5\n",
    "\n",
    "predictions_folders_1d = [\n",
    "    os.path.join(constants.PREDICTIONS, 'get_1d_conv_model_1d_final'),\n",
    "]\n",
    "predictions_folders_2d = [\n",
    "    os.path.join(constants.PREDICTIONS, 'get_general_2d_conv_model_2d_final')\n",
    "]\n",
    "name_result = '1d_2d_final_two_stage_{}_{}.csv'.format(weight_1d, weight_2d)\n",
    "make_two_stage_submission_file(\n",
    "    predictions_folders_1d,\n",
    "    predictions_folders_2d,\n",
    "    name_result,\n",
    "    weights=[weight_1d, weight_2d],\n",
    "    n_folds_1d=9,\n",
    "    n_folds_2d=10)\n",
    "print(name_result)\n",
    "\n",
    "del predictions_folders_1d, predictions_folders_2d\n",
    "del name_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
